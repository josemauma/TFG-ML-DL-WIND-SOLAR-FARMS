{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATE_TIME              0\n",
      "SOURCE_KEY             0\n",
      "DC_POWER               0\n",
      "AC_POWER               0\n",
      "DAILY_YIELD            0\n",
      "TOTAL_YIELD            0\n",
      "AMBIENT_TEMPERATURE    4\n",
      "MODULE_TEMPERATURE     4\n",
      "IRRADIATION            4\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cargar los datos de generación y meteorológicos\n",
    "plant_1_generation = pd.read_csv('Plant_1_Generation_Data.csv')\n",
    "plant_1_weather = pd.read_csv('Plant_1_Weather_Sensor_Data.csv')\n",
    "\n",
    "# Copiar los datos de generación para mantener df_GD1 limpio\n",
    "df_GD1 = plant_1_generation.copy()\n",
    "\n",
    "# Aplicar el mapeo a los nombres de los paneles solares\n",
    "unique_source_keys_list = df_GD1['SOURCE_KEY'].unique()\n",
    "source_key_mapping = {key: f\"Solar_Panel_{i+1}\" for i, key in enumerate(unique_source_keys_list)}\n",
    "df_GD1['SOURCE_KEY'] = df_GD1['SOURCE_KEY'].map(source_key_mapping)\n",
    "\n",
    "# Convertir la columna 'DATE_TIME' a formato de fecha y hora\n",
    "df_GD1['DATE_TIME'] = pd.to_datetime(df_GD1['DATE_TIME'], format='%d-%m-%Y %H:%M')\n",
    "plant_1_weather['DATE_TIME'] = pd.to_datetime(plant_1_weather['DATE_TIME'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Unir los datos meteorológicos al dataframe de generación en función de la fecha y hora\n",
    "df_GD1_with_weather = pd.merge(df_GD1, plant_1_weather, on='DATE_TIME', how='left')\n",
    "\n",
    "# Eliminar las columnas PLANT_ID_y y SOURCE_KEY_y y renombrar las columnas PLANT_ID_x y SOURCE_KEY_x\n",
    "df_GD1_with_weather_clean = df_GD1_with_weather.drop(columns=['PLANT_ID_y', 'SOURCE_KEY_y','PLANT_ID_x'])\n",
    "\n",
    "# Renombrar las columnas para eliminar el sufijo '_x'\n",
    "df_GD1_with_weather_clean = df_GD1_with_weather_clean.rename(columns={'SOURCE_KEY_x': 'SOURCE_KEY'})\n",
    "\n",
    "nulos = df_GD1_with_weather_clean.isnull().sum()\n",
    "\n",
    "# Mostrar cuántos valores nulos hay por columna\n",
    "print(nulos)\n",
    "\n",
    "# Rellenar los valores nulos en solo las columnas numéricas con la media\n",
    "numerical_cols = df_GD1_with_weather_clean.select_dtypes(include=['float64', 'int64']).columns #Estp para quitar los floats\n",
    "df_GD1_with_weather_clean[numerical_cols] = df_GD1_with_weather_clean[numerical_cols].fillna(df_GD1_with_weather_clean[numerical_cols].mean())\n",
    "\n",
    "df_GD1_limpio = df_GD1_with_weather_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores nulos en cada columna (planta 2):\n",
      " DATE_TIME              0\n",
      "SOURCE_KEY             0\n",
      "DC_POWER               0\n",
      "AC_POWER               0\n",
      "DAILY_YIELD            0\n",
      "TOTAL_YIELD            0\n",
      "AMBIENT_TEMPERATURE    0\n",
      "MODULE_TEMPERATURE     0\n",
      "IRRADIATION            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cargar los datos de generación y meteorológicos para la planta 2\n",
    "plant_2_generation = pd.read_csv('Plant_2_Generation_Data.csv')\n",
    "plant_2_weather = pd.read_csv('Plant_2_Weather_Sensor_Data.csv')\n",
    "\n",
    "# Copiar los datos de generación para mantener df_GD2 limpio\n",
    "df_GD2 = plant_2_generation.copy()\n",
    "\n",
    "# Aplicar el mapeo a los nombres de los paneles solares\n",
    "unique_source_keys_list_2 = df_GD2['SOURCE_KEY'].unique()\n",
    "source_key_mapping_2 = {key: f\"Solar_Panel_{i+1}\" for i, key in enumerate(unique_source_keys_list_2)}\n",
    "df_GD2['SOURCE_KEY'] = df_GD2['SOURCE_KEY'].map(source_key_mapping_2)\n",
    "\n",
    "# Convertir la columna 'DATE_TIME' a formato de fecha y hora\n",
    "# Ajustar el formato de fecha según sea necesario para que coincida con el archivo de la planta 2\n",
    "df_GD2['DATE_TIME'] = pd.to_datetime(df_GD2['DATE_TIME'], format='%Y-%m-%d %H:%M:%S')\n",
    "plant_2_weather['DATE_TIME'] = pd.to_datetime(plant_2_weather['DATE_TIME'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Unir los datos meteorológicos al dataframe de generación en función de la fecha y hora\n",
    "df_GD2_with_weather = pd.merge(df_GD2, plant_2_weather, on='DATE_TIME', how='left')\n",
    "\n",
    "# Eliminar las columnas PLANT_ID_y y SOURCE_KEY_y y renombrar las columnas PLANT_ID_x y SOURCE_KEY_x\n",
    "df_GD2_with_weather_clean = df_GD2_with_weather.drop(columns=['PLANT_ID_y', 'SOURCE_KEY_y', 'PLANT_ID_x'])\n",
    "\n",
    "# Renombrar las columnas para eliminar el sufijo '_x'\n",
    "df_GD2_with_weather_clean = df_GD2_with_weather_clean.rename(columns={'SOURCE_KEY_x': 'SOURCE_KEY'})\n",
    "\n",
    "# Mostrar cuántos valores nulos hay por columna\n",
    "nulos2 = df_GD2_with_weather_clean.isnull().sum()\n",
    "print(\"Valores nulos en cada columna (planta 2):\\n\", nulos2)\n",
    "\n",
    "# Rellenar los valores nulos en solo las columnas numéricas con la media\n",
    "numerical_cols_2 = df_GD2_with_weather_clean.select_dtypes(include=['float64', 'int64']).columns\n",
    "df_GD2_with_weather_clean[numerical_cols_2] = df_GD2_with_weather_clean[numerical_cols_2].fillna(df_GD2_with_weather_clean[numerical_cols_2].mean())\n",
    "\n",
    "# Guardar el dataframe limpio de la planta 2\n",
    "df_GD2_limpio = df_GD2_with_weather_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241us/step\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import joblib\n",
    "\n",
    "# Cargar los modelos y escaladores de cada planta\n",
    "Modelo_SP1 = load_model('Modelo_SP1.h5')\n",
    "scaler_ker1 = joblib.load('scaler_ker1.pkl')\n",
    "\n",
    "Modelo_SP2 = load_model('Modelo_SP2.h5')\n",
    "scaler_ker2 = joblib.load('scaler_ker2.pkl')\n",
    "\n",
    "features = ['AC_POWER', 'DAILY_YIELD', 'TOTAL_YIELD','AMBIENT_TEMPERATURE', \n",
    "            'MODULE_TEMPERATURE', 'IRRADIATION']\n",
    "# Definir X (variables predictoras) y Y (variable objetivo)\n",
    "X_ker_1 = df_GD1_limpio[features].values\n",
    "X_ker_2 = df_GD2_limpio[features].values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Planta 1: Divide en entrenamiento y prueba\n",
    "X_train_plant1, X_test_plant1, Y_train_plant1, Y_test_plant1 = train_test_split(X_ker_1, df_GD1_limpio['DC_POWER'].values, \n",
    "                                                                                test_size=0.2, random_state=42)\n",
    "\n",
    "# Planta 2: Divide en entrenamiento y prueba\n",
    "X_train_plant2, X_test_plant2, Y_train_plant2, Y_test_plant2 = train_test_split(X_ker_2, df_GD2_limpio['DC_POWER'].values, \n",
    "                                                                                test_size=0.2, random_state=42)\n",
    "\n",
    "# Función para realizar el ensamble\n",
    "def predict_ensemble(X_ker_1, X_ker_2):\n",
    "    min_samples = min(len(X_ker_1), len(X_ker_2))\n",
    "    X_ker_1_sub = X_ker_1[:min_samples]\n",
    "    X_ker_2_sub = X_ker_2[:min_samples]\n",
    "    \n",
    "    # Escalar y predecir con cada modelo\n",
    "    X_plant1_scaled = scaler_ker1.transform(X_ker_1_sub)\n",
    "    X_plant2_scaled = scaler_ker2.transform(X_ker_2_sub)\n",
    "    \n",
    "    pred_plant1 = Modelo_SP1.predict(X_plant1_scaled)\n",
    "    pred_plant2 = Modelo_SP2.predict(X_plant2_scaled)\n",
    "    \n",
    "    # Ensamble (promedio de las predicciones)\n",
    "    combined_prediction = (pred_plant1 + pred_plant2) / 2\n",
    "    \n",
    "    return combined_prediction\n",
    "\n",
    "\n",
    "# Ejemplo de predicción\n",
    "# X_test_plant1 y X_test_plant2 son las características de prueba de cada planta\n",
    "Y_pred_ensemble = predict_ensemble(X_test_plant1, X_test_plant2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples = min(len(X_test_plant1), len(X_test_plant2))\n",
    "\n",
    "# Reducir ambas matrices a un número común de muestras\n",
    "X_test_plant1_sub = X_test_plant1[:min_samples]\n",
    "X_test_plant2_sub = X_test_plant2[:min_samples]\n",
    "\n",
    "# Escalar los subconjuntos\n",
    "X_test_plant1_scaled = scaler_ker1.transform(X_test_plant1_sub)\n",
    "X_test_plant2_scaled = scaler_ker2.transform(X_test_plant2_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados del modelo ensamblado:\n",
      "MSE: 1128.3663189779902\n",
      "MAE: 19.04867500726633\n",
      "R²: 0.9997231401010581\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Seleccionar el tamaño mínimo de muestras entre ambas plantas\n",
    "min_samples = min(len(Y_test_plant1), len(Y_test_plant2))\n",
    "Y_test_plant1_sub = Y_test_plant1[:min_samples]\n",
    "Y_test_plant2_sub = Y_test_plant2[:min_samples]\n",
    "\n",
    "# Promediar los valores reales de ambas plantas para comparar con el ensamble\n",
    "Y_test_combined = (Y_test_plant1_sub + Y_test_plant2_sub) / 2\n",
    "\n",
    "# Calcular métricas de rendimiento\n",
    "mse = mean_squared_error(Y_test_combined, Y_pred_ensemble)\n",
    "mae = mean_absolute_error(Y_test_combined, Y_pred_ensemble)\n",
    "r2 = r2_score(Y_test_combined, Y_pred_ensemble)\n",
    "\n",
    "print(\"Resultados del modelo ensamblado:\")\n",
    "print(\"MSE:\", mse)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"R²:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultados del modelo ensamblado:\n",
    "\n",
    "MSE: 690.581889483876\n",
    "\n",
    "MAE: 15.555942529120001\n",
    "\n",
    "R²: 0.999830556416903"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
