{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATE_TIME              0\n",
      "SOURCE_KEY             0\n",
      "DC_POWER               0\n",
      "AC_POWER               0\n",
      "DAILY_YIELD            0\n",
      "TOTAL_YIELD            0\n",
      "PLANT_ID               4\n",
      "AMBIENT_TEMPERATURE    4\n",
      "MODULE_TEMPERATURE     4\n",
      "IRRADIATION            4\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cargar los datos de generación y meteorológicos\n",
    "plant_1_generation = pd.read_csv('Plant_1_Generation_Data.csv')\n",
    "plant_1_weather = pd.read_csv('Plant_1_Weather_Sensor_Data.csv')\n",
    "\n",
    "# Copiar los datos de generación para mantener df_GD1 limpio\n",
    "df_GD1 = plant_1_generation.copy()\n",
    "\n",
    "# Aplicar el mapeo a los nombres de los paneles solares\n",
    "unique_source_keys_list = df_GD1['SOURCE_KEY'].unique()\n",
    "source_key_mapping = {key: f\"Solar_Panel_{i+1}\" for i, key in enumerate(unique_source_keys_list)}\n",
    "df_GD1['SOURCE_KEY'] = df_GD1['SOURCE_KEY'].map(source_key_mapping)\n",
    "\n",
    "# Convertir la columna 'DATE_TIME' a formato de fecha y hora\n",
    "df_GD1['DATE_TIME'] = pd.to_datetime(df_GD1['DATE_TIME'], format='%d-%m-%Y %H:%M')\n",
    "plant_1_weather['DATE_TIME'] = pd.to_datetime(plant_1_weather['DATE_TIME'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Unir los datos meteorológicos al dataframe de generación en función de la fecha y hora\n",
    "df_GD1_with_weather = pd.merge(df_GD1, plant_1_weather, on='DATE_TIME', how='left')\n",
    "\n",
    "# Eliminar las columnas PLANT_ID_y y SOURCE_KEY_y y renombrar las columnas PLANT_ID_x y SOURCE_KEY_x\n",
    "df_GD1_with_weather_clean = df_GD1_with_weather.drop(columns=['SOURCE_KEY_y','PLANT_ID_x'])\n",
    "\n",
    "# Renombrar las columnas para eliminar el sufijo '_x'\n",
    "df_GD1_with_weather_clean = df_GD1_with_weather_clean.rename(columns={'SOURCE_KEY_x': 'SOURCE_KEY', 'PLANT_ID_y':'PLANT_ID'})\n",
    "\n",
    "nulos = df_GD1_with_weather_clean.isnull().sum()\n",
    "\n",
    "# Mostrar cuántos valores nulos hay por columna\n",
    "print(nulos)\n",
    "\n",
    "# Rellenar los valores nulos en solo las columnas numéricas con la media\n",
    "numerical_cols = df_GD1_with_weather_clean.select_dtypes(include=['float64', 'int64']).columns #Estp para quitar los floats\n",
    "df_GD1_with_weather_clean[numerical_cols] = df_GD1_with_weather_clean[numerical_cols].fillna(df_GD1_with_weather_clean[numerical_cols].mean())\n",
    "\n",
    "df_GD1_limpio = df_GD1_with_weather_clean;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores nulos en cada columna (planta 2):\n",
      " DATE_TIME              0\n",
      "SOURCE_KEY             0\n",
      "DC_POWER               0\n",
      "AC_POWER               0\n",
      "DAILY_YIELD            0\n",
      "TOTAL_YIELD            0\n",
      "PLANT_ID_y             0\n",
      "AMBIENT_TEMPERATURE    0\n",
      "MODULE_TEMPERATURE     0\n",
      "IRRADIATION            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cargar los datos de generación y meteorológicos para la planta 2\n",
    "plant_2_generation = pd.read_csv('Plant_2_Generation_Data.csv')\n",
    "plant_2_weather = pd.read_csv('Plant_2_Weather_Sensor_Data.csv')\n",
    "\n",
    "# Copiar los datos de generación para mantener df_GD2 limpio\n",
    "df_GD2 = plant_2_generation.copy()\n",
    "\n",
    "# Aplicar el mapeo a los nombres de los paneles solares\n",
    "unique_source_keys_list_2 = df_GD2['SOURCE_KEY'].unique()\n",
    "source_key_mapping_2 = {key: f\"Solar_Panel_{i+1}\" for i, key in enumerate(unique_source_keys_list_2)}\n",
    "df_GD2['SOURCE_KEY'] = df_GD2['SOURCE_KEY'].map(source_key_mapping_2)\n",
    "\n",
    "# Convertir la columna 'DATE_TIME' a formato de fecha y hora\n",
    "# Ajustar el formato de fecha según sea necesario para que coincida con el archivo de la planta 2\n",
    "df_GD2['DATE_TIME'] = pd.to_datetime(df_GD2['DATE_TIME'], format='%Y-%m-%d %H:%M:%S')\n",
    "plant_2_weather['DATE_TIME'] = pd.to_datetime(plant_2_weather['DATE_TIME'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Unir los datos meteorológicos al dataframe de generación en función de la fecha y hora\n",
    "df_GD2_with_weather = pd.merge(df_GD2, plant_2_weather, on='DATE_TIME', how='left')\n",
    "\n",
    "# Eliminar las columnas PLANT_ID_y y SOURCE_KEY_y y renombrar las columnas PLANT_ID_x y SOURCE_KEY_x\n",
    "df_GD2_with_weather_clean = df_GD2_with_weather.drop(columns=['SOURCE_KEY_y', 'PLANT_ID_x'])\n",
    "\n",
    "# Renombrar las columnas para eliminar el sufijo '_x'\n",
    "df_GD2_with_weather_clean = df_GD2_with_weather_clean.rename(columns={'SOURCE_KEY_x': 'SOURCE_KEY', 'PLANT_ID_Y':'PLANT_ID'})\n",
    "\n",
    "# Mostrar cuántos valores nulos hay por columna\n",
    "nulos2 = df_GD2_with_weather_clean.isnull().sum()\n",
    "print(\"Valores nulos en cada columna (planta 2):\\n\", nulos2)\n",
    "\n",
    "# Rellenar los valores nulos en solo las columnas numéricas con la media\n",
    "numerical_cols_2 = df_GD2_with_weather_clean.select_dtypes(include=['float64', 'int64']).columns\n",
    "df_GD2_with_weather_clean[numerical_cols_2] = df_GD2_with_weather_clean[numerical_cols_2].fillna(df_GD2_with_weather_clean[numerical_cols_2].mean())\n",
    "\n",
    "# Guardar el dataframe limpio de la planta 2\n",
    "df_GD2_limpio = df_GD2_with_weather_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Plant_1       0.85      0.96      0.90     13825\n",
      "     Plant_2       0.95      0.82      0.88     13471\n",
      "\n",
      "    accuracy                           0.89     27296\n",
      "   macro avg       0.90      0.89      0.89     27296\n",
      "weighted avg       0.90      0.89      0.89     27296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Supongo que ya has preprocesado los datos de ambas plantas y los has combinado\n",
    "df_combined = pd.concat([df_GD1_limpio, df_GD2_limpio], ignore_index=True)\n",
    "\n",
    "# Añadir una columna 'ZONE' para indicar la planta utilizando 'PLANT_ID'\n",
    "df_combined['ZONE'] = df_combined['PLANT_ID'].apply(lambda x: 1 if x == 4135001 else 2)\n",
    "\n",
    "# Separar características (X) y etiquetas (y)\n",
    "features = ['AC_POWER', 'DAILY_YIELD', 'TOTAL_YIELD','AMBIENT_TEMPERATURE', \n",
    "            'MODULE_TEMPERATURE', 'IRRADIATION']\n",
    "# Definir X (variables predictoras) y Y (variable objetivo)\n",
    "X = df_combined[features].values\n",
    "y = df_combined['ZONE']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Escalar las características\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Crear el modelo de Regresión Logística\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluar el modelo\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Generar un informe de clasificación\n",
    "report = classification_report(y_test, y_pred, target_names=['Plant_1', 'Plant_2'], labels=[1, 2])\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                  precision   recall   f1-score  support\n",
    "\n",
    "     Plant_1       0.85      0.96      0.90     13825\n",
    "     Plant_2       0.95      0.82      0.88     13471\n",
    "\n",
    "      accuracy                           0.89     27296\n",
    "      macro avg     0.90      0.89      0.89     27296\n",
    "      weighted avg   0.90      0.89      0.89     27296\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Plant_1       0.85      0.96      0.90     13825\n",
      "     Plant_2       0.95      0.82      0.88     13471\n",
      "\n",
      "    accuracy                           0.89     27296\n",
      "   macro avg       0.90      0.89      0.89     27296\n",
      "weighted avg       0.90      0.89      0.89     27296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RF_model = RandomForestClassifier(n_estimators=150, random_state=42)\n",
    "\n",
    "RF_model.fit(X_train_scaled,y_train)\n",
    "RF_y_pred = RF_model.predict(X_test_scaled)\n",
    "\n",
    "report = classification_report(y_test, y_pred, target_names=['Plant_1', 'Plant_2'], labels=[1, 2])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Puntajes de la validación cruzada: [1.         1.         1.         0.99990841 1.        ]\n",
      "Media de la validación cruzada: 1.000\n"
     ]
    }
   ],
   "source": [
    "CV_model = cross_val_score(RF_model, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "# Imprimir los resultados de la validación cruzada\n",
    "print(f'Puntajes de la validación cruzada: {CV_model}')\n",
    "print(f'Media de la validación cruzada: {np.mean(CV_model):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo no tiene el atributo 'feature_importances_'. Asegúrate de que estás usando un modelo que soporte esta funcionalidad, como RandomForestClassifier.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    importances = model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    features = df_combined.drop(['ZONE', 'PLANT_ID'], axis=1).columns\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(\"Importancia de las Características en el Modelo de Random Forest\")\n",
    "    plt.bar(range(X.shape[1]), importances[indices], align=\"center\")\n",
    "    plt.xticks(range(X.shape[1]), features[indices], rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except AttributeError:\n",
    "    print(\"El modelo no tiene el atributo 'feature_importances_'. Asegúrate de que estás usando un modelo que soporte esta funcionalidad, como RandomForestClassifier.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
